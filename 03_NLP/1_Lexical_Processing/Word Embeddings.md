## Introduction

We all know that when it comes to Train a model for a NLP Problem Statement, there is a definitive need for various Preprocessing Techniques to process Textual Data such that the Data can be provided to the Model for Training.

One of the most important Preprocessing Technique is ***Converting Textual Data to Vector/Numerical Data***

In Traditional Technique of Converting Textual Data into Numerical Data, we have many techniques such as Bag-of-Words, One Hot Encoding, Mean Encoding, etc.

With these Techniques, we faced a lot of issues like High Dimensions, Loss of Semantic/Syntactic Information, etc.

Therefore, researchers came up with Word Embeddings Technique where we Convert Textual Data into Vector Data.
Some Techniques are like TF-IDF, Word2Vec, CBOW, Skip-gram, NGRAM Models, etc.
## Working of Word Embedding Layer in Keras

There are multiple steps which are performed in Keras for Word Embedding Layers. Let's discuss and understand them based on the Assumptions we are taking for this Example.

Assumptions:
- From a Dataset of many Sentences for this Example we will select only one Sentence i.e. "Boy is good".
- Vocabulary Size is 10K because when all the Sentences from the Dataset are combined, the total unique words are 10K.
- These 10K words will be stored as a Dictionary in a sorted manner such that for each Word/Token the index will be stored as a Value. Say the word "Boy" came at 2000 index when sorted and stored in a Dictionary.

At first, for each token of the selected Sentence a `One-Hot Encoding` will be done such that we will get 3 Sparse Matrices for 3 Tokens. As the Vocabulary Size if 10K so each Matrix will be of 10K length and 1 will be represented at the Index No. of the Token like in the Matrix for the token Boy which is at 2K Index 1 will only be placed at 2K Index rest all Zero. Same will happen for the other two tokens.

Now, we will convert the Sparse Matrix into a Dense Matrix. By this we mean that we will create a new matrix with total length of 3 and each word will only be represented by the Index Number.
So, the Dense Matrix for the Sentence "Boy is good" will be [2000, 3345, 7502].
>2000 -> Boy
>3345 -> is
>7502 -> good

Now after this, we will have to pass the value for `Output Dimension` parameter. Say we provide 32 Dimensions.
So, the number 2000 which represents Boy, will now be represented by a 32 Dimension Vector with floating numbers based on the predictions of a pre-trained model. This model is trained on a huge corpus of words to find out if the two words are related or not. So, the 32 Dimensions which we have provided will actually become 32 Categories such are Gender, Royal, Age, Food, Appreciation, etc. And based on the relatedness of the word Boy with each Categorical Dimension a floating number will be provided. This is how a new Vector of 32 Dimensions will finally represent the word `Boy`. Using this technique helps us in identifying what words are similar and what are not. This Categorical Dimension Relatedness Based Technique is also called as ***Feature Representation***.

![[Pasted image 20240620225015.png]]
## Categories of Word Embeddings

There are two Main Categories of Word Embeddings:
1. **Frequency-based Embedding:** Embedding methods that utilize the Frequency of Words to generate their Vector Representations are known as Frequency-based Embeddings. Frequency-based methods use Statistical Measures of how often words appear in the corpus to encode Semantic Information.
2. **Prediction-based Embeddings:** Generated by models that learn to predict words from their neighboring words in Sentences. These methods aim to position Words with Similar Contexts Closely together in the Embedding Space.

Modern NLP Models, such as the Transformer Architecture, typically use ***Prediction-based Embeddings***.
## Word2Vec

In this specific model, each word is basically represented as a vector of 32 or more dimensions instead of a single number, like in 2D a word Man can we written as (3, 6) & Woman as (3.2, 6.2).

The words which are related to each other or are used together frequently are near to each other.

Advantage of ***Word2Vec*** Model is that it is able to preserve the Semantic Information & Relation between different words.

Steps to Create Word2Vec:
- Tokenization of the Sentences
- Create Histograms
- Take most frequent words
- Create a Matrix with all the unique words. It also represent the occurrence relation between the words.

Word2Vec Model can be used from the ***Gensim*** Library in Python.

```
from gensim.models import Word2Vec
```

if not already installed, then use:
```
pip install gensim
```

As input to the Word2Vec Function, we give a list having each index as a different sentence and at each index a list of tokens are stored. Also we give a numeric value to `min_count` argument which states that if a word is less than the specified `min_count` then it will be ignored and no vectors will be assigned for that word/token. For example, if we give `min_count=2` then it means that any word which has a frequency of less than 2 then Word2Vec Model won't assign Vectors for that word/token.

Code to get a dictionary for the words for which the Vectors are assigned:
```
words = model.wv.vocab
```

Code to get the Vectors for a specified word/token:
```
vec = model.wv["WORD"]
```

Code to extract the most similar words for a specified Word:
```
similar_words = model.wv.most_similar("WORD")
```


Note:
- Sparse Matrix refers to the Matrix that consists of high No. of Zero's but very less No. of Ones.
- Word Embeddings Layer Implementation using Keras is Done on Google Colab.
